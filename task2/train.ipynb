{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Dataset, Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, global_max_pool\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('/home/jiangzhengqun/ml/task2/bert_model')\n",
    "\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir):\n",
    "        super().__init__()\n",
    "        self.labels = []\n",
    "        self.file_paths = []\n",
    "        self.inputs = []\n",
    "        self.root_dir = root_dir\n",
    "        \n",
    "        # 读取 CSV 文件，获取文件名、标签和输入序列\n",
    "        with open(csv_file, mode='r') as file:\n",
    "            csv_reader = csv.reader(file)\n",
    "            next(csv_reader)  # 跳过表头\n",
    "            for row in csv_reader:\n",
    "                file_name, input_sequence, label = row\n",
    "                self.file_paths.append(os.path.join(root_dir, file_name))\n",
    "                self.labels.append(float(label))  # 根据需要调整数据类型\n",
    "                tokenized_inputs = tokenizer(input_sequence, padding='max_length', truncation=True, return_tensors='pt', max_length=10)\n",
    "                for key in tokenized_inputs:\n",
    "                    tokenized_inputs[key] = tokenized_inputs[key].squeeze(0)\n",
    "                self.inputs.append(tokenized_inputs) # 读取输入序列\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return self.file_paths\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return self.file_paths\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with open(self.file_paths[idx], 'rb') as f:\n",
    "            graph = pickle.load(f)\n",
    "\n",
    "        # 创建图数据对象\n",
    "        node_type = torch.tensor(graph['node_type'], dtype=torch.float)\n",
    "        num_inverted_predecessors = torch.tensor(graph['num_inverted_predecessors'], dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # 拼接两个特征矩阵\n",
    "        x = torch.cat([node_type, num_inverted_predecessors], dim=1)\n",
    "        data = Data(\n",
    "            x=x,\n",
    "            edge_index=torch.tensor(graph['edge_index'], dtype=torch.long),\n",
    "            num_nodes=int(graph['nodes'])\n",
    "        )\n",
    "        # 添加标签\n",
    "        data.y = torch.tensor([self.labels[idx]], dtype=torch.float)\n",
    "        \n",
    "        return data, self.inputs[idx]\n",
    "\n",
    "\n",
    "class GCNBERTModel(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(GCNBERTModel, self).__init__()\n",
    "        self.gcn1 = GCNConv(num_features, 16)\n",
    "        self.gcn2 = GCNConv(16, 32)\n",
    "        self.bert_fc = nn.Linear(768, 32)\n",
    "        self.fc_combined = nn.Sequential(\n",
    "            nn.Linear(32 * 2, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        self.bert_model = BertModel.from_pretrained('/home/jiangzhengqun/ml/task2/bert_model')\n",
    "\n",
    "    def forward(self, graph_data, tokenized_inputs):\n",
    "        # GCN processing\n",
    "        x, edge_index, batch = graph_data.x, graph_data.edge_index, graph_data.batch\n",
    "        x = self.gcn1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.gcn2(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        # BERT processing\n",
    "        bert_outputs = self.bert_model(**tokenized_inputs)\n",
    "        sequence_output = bert_outputs.last_hidden_state\n",
    "        sequence_emb = torch.mean(sequence_output, dim=1)\n",
    "        sequence_emb = self.bert_fc(sequence_emb)\n",
    "\n",
    "        # Combine GCN and BERT outputs\n",
    "        combined = torch.cat([x, sequence_emb], dim=1)\n",
    "        out = self.fc_combined(combined)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "train_file = '/home/jiangzhengqun/ml/dataset2/task2_train_data.csv'\n",
    "train_dir = '/home/jiangzhengqun/ml/dataset2/task2_train_data'\n",
    "train_set = GraphDataset(csv_file=train_file, root_dir=train_dir)\n",
    "train_loader = DataLoader(train_set, batch_size=128)\n",
    "\n",
    "test_file = '/home/jiangzhengqun/ml/dataset2/task2_test_data.csv'\n",
    "test_dir = '/home/jiangzhengqun/ml/dataset2/task2_test_data'\n",
    "test_set = GraphDataset(csv_file=test_file, root_dir=test_dir)\n",
    "test_loader = DataLoader(test_set, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "model = GCNBERTModel(num_features=2)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss for regression\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "criterion = torch.nn.MSELoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_674186/2870820199.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  node_type = torch.tensor(graph['node_type'], dtype=torch.float)\n",
      "/tmp/ipykernel_674186/2870820199.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  num_inverted_predecessors = torch.tensor(graph['num_inverted_predecessors'], dtype=torch.float).unsqueeze(1)\n",
      "/tmp/ipykernel_674186/2870820199.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index=torch.tensor(graph['edge_index'], dtype=torch.long),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after epoch 1 the train_loss is 0.07283311248432066\n",
      "after epoch 1 the eval_loss is 0.08024843511256305\n",
      "after epoch 2 the train_loss is 0.05655975732140157\n",
      "after epoch 2 the eval_loss is 0.07324079378039725\n",
      "after epoch 3 the train_loss is 0.05512824948460167\n",
      "after epoch 3 the eval_loss is 0.05598431573757394\n",
      "after epoch 4 the train_loss is 0.05445767522296803\n",
      "after epoch 4 the eval_loss is 0.05579523812048137\n",
      "after epoch 5 the train_loss is 0.054217128901401276\n",
      "after epoch 5 the eval_loss is 0.06195736669575457\n",
      "after epoch 6 the train_loss is 0.05424709628717866\n",
      "after epoch 6 the eval_loss is 0.054949718998448756\n",
      "after epoch 7 the train_loss is 0.053776135719597856\n",
      "after epoch 7 the eval_loss is 0.06177992922973565\n",
      "after epoch 8 the train_loss is 0.054321070408805947\n",
      "after epoch 8 the eval_loss is 0.05716705598487434\n",
      "after epoch 9 the train_loss is 0.05415920694331573\n",
      "after epoch 9 the eval_loss is 0.05318928686690263\n",
      "after epoch 10 the train_loss is 0.05390553713402559\n",
      "after epoch 10 the eval_loss is 0.06184062943859978\n",
      "after epoch 11 the train_loss is 0.05361178175608498\n",
      "after epoch 11 the eval_loss is 0.05199070865373043\n",
      "after epoch 12 the train_loss is 0.053576658789859004\n",
      "after epoch 12 the eval_loss is 0.05605404712365602\n",
      "after epoch 13 the train_loss is 0.05351669220172605\n",
      "after epoch 13 the eval_loss is 0.05535573908657005\n",
      "after epoch 14 the train_loss is 0.054288052193019104\n",
      "after epoch 14 the eval_loss is 0.05346503970742395\n",
      "after epoch 15 the train_loss is 0.05391006088182113\n",
      "after epoch 15 the eval_loss is 0.06546824315922674\n",
      "after epoch 16 the train_loss is 0.053598175674059625\n",
      "after epoch 16 the eval_loss is 0.07574259405109016\n",
      "after epoch 17 the train_loss is 0.05387807914413252\n",
      "after epoch 17 the eval_loss is 0.06401371251029725\n",
      "after epoch 18 the train_loss is 0.053833484221899576\n",
      "after epoch 18 the eval_loss is 0.12353186749599197\n",
      "after epoch 19 the train_loss is 0.05354700470478599\n",
      "after epoch 19 the eval_loss is 0.07281481614336371\n",
      "after epoch 20 the train_loss is 0.053459793999303826\n",
      "after epoch 20 the eval_loss is 0.06679675105459121\n",
      "after epoch 21 the train_loss is 0.05416698660295413\n",
      "after epoch 21 the eval_loss is 0.06012536771595478\n",
      "after epoch 22 the train_loss is 0.053640376909116154\n",
      "after epoch 22 the eval_loss is 0.05966391546694054\n",
      "after epoch 23 the train_loss is 0.05301845367919372\n",
      "after epoch 23 the eval_loss is 0.07770577823976055\n",
      "after epoch 24 the train_loss is 0.05263937575482698\n",
      "after epoch 24 the eval_loss is 0.054553831165487114\n",
      "after epoch 25 the train_loss is 0.052533335124174955\n",
      "after epoch 25 the eval_loss is 0.06688380884853276\n",
      "after epoch 26 the train_loss is 0.052945967061109406\n",
      "after epoch 26 the eval_loss is 0.06363460529510948\n",
      "after epoch 27 the train_loss is 0.052637826517321774\n",
      "after epoch 27 the eval_loss is 0.06227361992932856\n",
      "after epoch 28 the train_loss is 0.0524667092926562\n",
      "after epoch 28 the eval_loss is 0.07378382282331586\n"
     ]
    }
   ],
   "source": [
    "# writer = SummaryWriter('./runs')\n",
    "writer = SummaryWriter('./run_2024_6_7')\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    n_loss = 0\n",
    "    for data, tokenized_inputs in train_loader:\n",
    "        data.to(device)\n",
    "        for key in tokenized_inputs:\n",
    "            tokenized_inputs[key] = tokenized_inputs[key].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data, tokenized_inputs).squeeze(1)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        n_loss += 1\n",
    "    train_loss = epoch_loss / n_loss\n",
    "    print(f'after epoch {epoch + 1} the train_loss is {train_loss}')\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    n_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, tokenized_inputs in test_loader:\n",
    "            data.to(device)\n",
    "            for key in tokenized_inputs:\n",
    "                tokenized_inputs[key] = tokenized_inputs[key].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data, tokenized_inputs).squeeze(1)\n",
    "            loss = criterion(out, data.y)\n",
    "            epoch_loss += loss.item()\n",
    "            n_loss += 1\n",
    "    eval_loss = epoch_loss / n_loss\n",
    "    writer.add_scalars('Loss', {'train': train_loss, 'valid': eval_loss}, epoch + 1)\n",
    "    print(f'after epoch {epoch + 1} the eval_loss is {eval_loss}')\n",
    "    # 保存模型参数\n",
    "    if (epoch + 1) % 100 == 0 or (epoch + 1) == num_epochs:\n",
    "        torch.save(model.state_dict(), f'./model1/model_epoch_{epoch + 1}.pth')\n",
    "writer.close()\n",
    "# 在训练结束时保存最终的模型参数\n",
    "torch.save(model.state_dict(), './model1/model_final.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
